# 游戏中多点事务的思考

- Q：
    目前游戏中大部分逻辑不需要用到类似 2PC 的共识算法。为什么呢？
- A：
    目前大部分跨服请求不需要征求目标节点的同意（如向无状态的中心服跨服，如跨服通知跨服聊天），本质上是主节点一个人决定的事务，而不是共识。所以只管不停重试，其他节点一定同意且早晚执行，因此也不存在回滚。这里只要留意重复执行，注意跨服请求的幂等性就可以了。
    与之相对，如果需要另一节点也提出意见，事情就变得复杂了。甚至，如果有超过两个节点，情况就变成了典型的共识算法场景。

我们在下面讨论这些复杂情况。

跨服好友添加
===
以加好友为例，两点事务一共只有三步：
1. 主节点检查，并通知从节点。
2. 从节点检查并直接修改数据，且回报主节点。
3. 发起方修改数据。
虽然比起经典的 2PC 少了一步，但本质上还是 2PC。因为这里需要检查的从节点只有一个，它可以立即修改自身的数据，所以相当于在从节点合并了 2PC 中的表决和提交两步。同时，省略了超时以及通知回滚的逻辑，所有的不一致在之后不定期修复。
在简单的系统，这其实是一个可行的思路。

跨服亲密度修改
===
更进一步，如果是修改亲密度这种不止两个状态的操作呢？
我们仍然可以使用前述偷懒思路。但是在修复不一致状态时也许需要更多的信息，也许需要引入增删操作版本号来完成。
这里维护版本号其实也是一个分布式系统中维护事件顺序的问题。由于我们这里只有两点和一种事务，所以情况变得很简单。


三节点及以上事务怎么办？？？
===
已经不能用前面的偷懒算法，必须使用真正的共识算法了。

2PC/TCC
---
原标准算法咱不再赘述。
实现 2PC/TCC，事务参与者实现（一阶段）预留和（回滚时）释放资源的操作。大部分游戏业务中因为错误暂时锁定一部分资源（钱，好友/资源坑位之类的）或发生不一致是可以容忍的，所以我们的算法也许可以设法容忍 2PC/TCC 中无解的不一致情况。
我们还需要确保所有节点不会失忆。如果不能保证，也许需要重新讨论。

算法如下：
1. 流程和 2PC/TCC 一致。
2. 针对主失效，由于在游戏中主通常有逻辑上的意义（通常为发起操作的人）且我们对事务失败可以容忍（玩家可以重新操作一次），故不再选举新主。
3. 由于不重新选举新主，我们的从节点也需要计时回滚事务。
4. 活着的节点可以通过 ACK 感知到其他节点的宕机和网络错误。利用这一点我们可以增加重发减少通信失败的可能性，也可以利用这些信息驱动修复逻辑。我们后面再讨论有后者恢复逻辑的情况。

这里以三人结义为例子，讨论 2PC/TCC 中会遇到的经典失效问题：
- 阶段1从挂掉：
    这不成问题。主可以超时通知其他从放弃结义。各个从也可以自己超时放弃。
- 阶段1主挂掉：
    从节点各自超时放弃。主重启发现超时后也放弃。
- 阶段2从挂掉，或没收到主的提交通知：
    主和其他从记录了结义，而挂掉或断网的从自己放弃了结义。不一致发生了！不过我们可以在之后修复这种情况，只要有一人认为结义成功，就补上这个信息。
- 阶段2主挂掉，不过在此之前已经有从收到提交通知了：
    和上面类似，之后修复即可。
- 阶段2主挂掉，且没有从知道成功了：
    这个情况对于 2PC 来说是无解情况。无论剩下节点如何操作都可能和已执行操作不一致。这也是 3PC 要解决的问题。但由于我们业务上的宽容和修复，这种情况并不可怕。
    其他从自己默默回滚了。在我们的业务中可以假设主可以最终回归。最终还是使用上述修复逻辑。

上面逻辑存在一个问题：阶段2的所有错误情况都可能造成部分节点暂时以为自己放弃了事务。我们最终会修复该情况，但在修复前的窗口期这个人跑去消耗了释放的资源（如把结义名额用完了），就会出现不可修复的不一致！我们讨论以下措施：
- 若我们对预留锁定资源容忍度大，那么我们可以延长子节点自动放弃的时间甚至永不放弃，如此一直占用资源确保资源不会被错误消耗。
- 当从身上有前述未完成事务，可以利用事务中的信息反复询问其他节点，加快修复的速度。
- 主也可以记录通知提交/回滚的确认状态。在所有从返回确认之前，它可以反复向未确认的从同步事务结果。

当然，上述的解决方案都假设主会（短时间内）回归。在游戏业务中，这样的假设并不过分(〃'▽'〃)

在上面措施的基础上剩下的问题：

1. 在绝少情况下，参与事务的节点还是会出现分歧。例如，有超时取消的情况下，经典失效问题中的最后一个情况中，活着的节点询问无果超时后全部回滚并释放预留的资源。这相当于我们的主节点回归假设没有满足... 此时不得不引入 3PC 甚至 Paxos。不过考虑到这种情况绝少见，我们可以选择人工解决该问题。
2. 如果考虑节点会失忆的情况，那么会十分棘手。在我们的大多数逻辑中，为了简便，我们修复共识时不使用少数服从多数，而是有节点认为事务已完成那么所有节点都应这么认为。按照这个逻辑，在触发修复后我们最终可以使数据一致。
3. 算法本身保证得太少，业务层的修复逻辑太多。这样的设计很难做一个漂亮的通用框架出来。

3PC
---
3PC 是为了解决 2PC 主在阶段二挂掉后，活着的节点无人知道是否应该提交的情况。在前文我们已讨论对于这些情况的解决方式。故实际上我们引入 3PC 的必要性不大。

消息事务
---
参考 https://www.cnblogs.com/chjxbt/p/11412727.html
附带 MQ 相关 https://www.cnblogs.com/rjzheng/p/8994962.html
和游戏中的情况有微妙的相似（或者说完全一样），都是假设消息消费者节点不可以否决事务，消息未被消费就不断重试。它的特点是**异步**，发起事务的节点只管自己成功提交就好了，确保从节点也完成事务的工作交给 MQ 进行。
它是对一致性的实时性要求不高和能容忍异步时的解法，规则很宽松，可以提升性能。也是 kafka 和 rocketMQ 支持的特性。

这里的重点变成了：
- 确保发起者把消息送上 MQ 且不重复发消息：
    MQ 增加 ACK。MQ 检查 token 或者类似 TCP 的 seq 避免重复送消息。
- 确保 MQ 不丢消息：
    MQ 自己设法保证持久化和健壮性，MQ 本身使用集群、同步主从复制等。
- 确保 MQ 把消息送给消费者且不重复消费：
    引入消费 ACK。在逻辑层面保证幂等或引入 token 或者类似 TCP 的 seq。

这里比较一下，2PC 和 TCC 是确保事务一起提交，是同步的；而消息事务是分别提交，异步的。